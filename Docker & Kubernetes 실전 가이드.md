# Docker & Kubernetes: 실전 가이드

출처: Docker & Kubernetes: 실전 가이드 -2022년판

https://www.udemy.com/course/docker-kubernetes-2022/?src=sac&kw=Docker+%26+K

## 섹션 1: 시작하기

### Docker란?

- 컨테이너 기술. 컨테이너를 생성하고 관리하기 위한 도구.

- 컨테이너

  - 소프트웨어 개발에서 컨테이너란 `표준화된 소프트웨어 유닛`. 

  - 코드 패키지이며 해당 코드를 실행하는데 필요한 종속성(dependencies)과 도구가 포함
  - 예를들어 nodejs라면 소스 코드와 nodejs런타임, 코드 실행에 필요한 도구들이 있다. 동일한 코드, 도구, 같은 버전의 nodejs런타임을 사용하는 동일한 컨테이너는 `항상 동일한 동작과 결과를 제공`한다.
  - 항구에서 사용하는 컨테이너는 한 컨테이너에 보관하는 물건들은 다른 컨테이너와 섞이지 않고 트럭에 실을 수 있다. 도커도 이처럼 소프트웨어 유닛, 코드가 포함된 패키지 및 코드 실행하는 종속성을 보관할 수 있고 도커가 실행되는 곳에서 가져올 수 있다.
  - 모든 것은 컨테이너에 있기 때문에 애플리케이션을 실행하려는 위치에 추가 도구를 설치하는 것에 걱정할 필요 없다. 컨테이너를 만드는 것이 사실상 표준으로 컨테이너 지원은 최신 운영 체제에 내장되었거나 설치해 작업할 수 있다.
  - 도커는 결국 **컨테이너 생성 및 관리 프로세스를 단순화하는 도구**다. 



### 왜 Docker와 컨테이너인가?

- 소프트웨어 개발에서 독립적이고 표준화된 애플리케이션 패키지를 원하는 이유는 무엇일까? 우리는 종종 다른 개발 & 생산 환경을 가진다. nodejs 애플리케이션을 생성하고 특정 버전에서 실행되는 코드를 작성한다. 그런데 그 버전이 로컬에만 설치되어 있다면 호스트되어야 하는 서버의 일부 원격 시스템에 배포할 때 이전 버전의 nodejs가 존재할 수 있다. 그러면 코드가 동작하지 않게 되고 개발과 생산에서 같은 환경은 중요하다.
- 따라서 특정 버전을 도커 컨테이너에 고정(lock)할 수 있어 항상 정확한 버전으로 코드가 실행되게 할 수 있다. 
- 또 팀, 회사 내의 각각의 개발 환경이 있을 때 서로 다른 버전에서 코드를 작성하고 공유해야 한다면 코드가 필요한 모든 것을 포함해 공유하는게 좋다.
- 혼자 작업할 때도 유용하다. 작업 중인 프로젝트가 여러개라면 충돌하는 버전이 있을 수 있다. 작업을 전환할 때 버전을 제거할 필요 없이 각 버전을 컨테이너에 보유해 각 프로젝트가 자기 버전을 가지도록 한다. 



### 가상 머신 vs Docker 컨테이너

- 가상 머신으로 해결하면 안될까?
- 가상 머신은` 호스트 운영 체제에 독립적인 자체 쉘(shell)을 지닌 캡슐화된 가상 운영 체제`를 가진다. 가상 머신은 우리가 사용하는 운영 체제 위에 설치하는 컴퓨터 내부의 컴퓨터다. 컴퓨터처럼 원하는 것을 우리가 설치할 수 있기 때문에 여기로 코드를 옮겨 실행하거나 동료와 환경을 공유할 수 있지만 문제는 가상 운영 체제를 가진 여러 가상 머신에서 `오버헤드가 발생`한다. 또한 여러 가상 머신에서 같은 환경을 공유해도 각각의 머신에 따로 설치해야한다. 예를 들어 리눅스 운영체제를 모든 가상 머신에서 사용해도 모두 따로 설치해야한다. 또 필요하지 않지만 기본적으로 설치된 기본 도구들도 문제가 된다.
- 장점: 분리된 환경, 환경별 구성, 안정적인 공유와 재생산
- 단점: 중복 복제로 메모리, cpu, 드라이브 공간 낭비. 호스트 시스템 위의 추가 시스템으로 인한 성능 저하, 재생산 및 공유가 가능해도 `단일 구성 파일 없어` 원하는 모든 시스템에 가상 머신을 설정하고 정확히 동일하게 구성해야해 까다로움.
- 핵심은 **컨테이너**고 도커는 이를 만들고 관리하기 위한 표준 도구일 뿐이다.
- 반면 도커는 호스트 운영 체제가 존재하지만 하나의 머신에 몇 개를 설치하지 않는다. 운영체제가 기본적으로 내장하거나 컨테이너 에뮬레이트를 지원하는 내장 컨테이너를 활용해 (Emulated Container Support) 도커가 이를 동작시킨다. 그 위에 `도커 엔진`이라는 도구를 설치하고 그리고 이를 기반으로 컨테이너를 가동한다. 컨테이너를 여러 개로 분리해 기동도 가능하다. 컨테이너 내부에는 작은 운영 체제 레이어가 있을 수 있지만 `가상 머신에 설치하는 것보다 훨씬 작은 운영 체제`의 매우 가벼운 버전이다. 구성 파일을 사람들과 공유해 다른 사람들도 만들거나 이미지라고 불리는 것에 빌드할 수 있다. 그 이미지를 다른 사람과 공유해 모든 사람이 자신의 시스템에서 우리 시스템에 있는 동일한 컨테이너를 시작할 수 있게 한다. 
- 장점: 운영체제와 시스템에 미치는 영향이 적고 매우 빠르며 최소한의 디스크 공간 사용. 이미지, 구성 파일이 존재해 공유, 재구축, 배포가 매우 쉬움. 앱에 필요한 모든 걸 캡슐화해 머신 전체에 필요한 부가적인 것들이 없다.
- 버추얼 머신은 운영체제에 많은 영향을 끼치고 느리고 디스크 공간을 많이 차지한다.

### Docker 설정

- mac, windows는 요구사항을 충족할 경우 docker desktop, 충족하지 못할 경우 docker toolbox를 설치. linux는 docker engine 바로 설치 가능. linux는 도커 기술과 컨테이너를 기본 지원한다.
- powershell을 관리자 권한으로 실행해 wsl --install로 설치 후 ubuntu, docker설치

- Docker Hub: 클라우드, 웹에서 이미지를 호스팅해 다른 시스템, 사람들에게 쉽게 공유해주는 서비스



### 실전에 참여하기

- 컨테이너는 항상 이미지를 기반으로 하기 때문에 먼저 생성해야 한다.

```dockerfile
FROM node:14 #Nodejs를 기본 이미지로 활용

WORKDIR /app # 컨테이너 파일 시스템 내의 특정 디렉토리

COPY package.json . # package.json을 작업 디렉토리에 복사

RUN npm install # 종속성 설치

COPY . . # 나머지 코드 복사

EXPOSE 3000 # 포트 3000을 외부에 노출. 우리 애플리케이션이 수신 대기하는 포트로 컨테이너 내부, 외부에서 이 포트에 도달할 수 있게 한다.

CMD [ "node", "app.mjs" ] #node 명령으로 실행
```

- Dockerfile작성 후 터미널에서 `docker build . `실행하면 프로젝트 디렉토리에서 찾아 Dockerfile 이미지 빌드. 성공하면 이미지 결과물 id를 얻는다. docker run {id}를 실행해야 하지만 실제로 컨테이너는 통신하려는 포트가 있으므로 실행 컨테이너에 포트를 게시해야 한다.
- `docker run -p 3000:3000 {id}`는 포트 3000에 포트 3000을 게시한다는 의미다. 로컬 시스템의 로컬 호스트를 사용해 컨테이너 대신 포트 3000에서 사용하는 애플리케이션에 연결할 수 있다. 컨테이너와 호스트 운영 체제 사이에는 디폴트 연결이 없기 때문이다. 예를 들어 컨테이너에서 실행 중인 애플리케이션에 http 요청을 보내려면 통신하려는 컨테이너의 포트를 열어야 한다. 그렇지 않으면 컨테이너에 잠긴 네트워크이므로 외부에서 연결할 수 없다.

- 실행중인 터미널인 아닌 새로운 터미널을 열어 docker ps로 현재 실행중인 컨테이너를 확인하고 자동으로 부여된 이름을 볼 수 있다. 그리고 docker stop {name}으로 중단시킨다.



## 섹션 2: Docker 이미지 & 컨테이너: 코어 빌딩 블록

- 이미지: 디졸버(dissolver) 개념으로 템플릿, 컨테이너의 청사진이 된다. `실제 모든 코드와 코드를 실행하는데 필요한 도구인 설정 명령을 포함`한 공유 가능한 패키지다. 그 다음 컨테이너가 실행되어 코드를 실행한다. `컨테이너는 이미지의 구체적인 실행 인스턴스`다. 이미지를 사용해 여러 컨테이너를 만든다. 한번 이미지를 정의하면 다른 시스템, 서버에서 여러 번 실행 가능하다. 

- 이미 존재하는 이미지 사용: 동료 구축 or 일반적이어서 미리 구축된 이미지(Docker Hub).

- - 내가 택한 폴더에서 도커허브에서 찾은 명령어 실행해서 컨테이너가 만들어지면 `docker ps -a` 실행. ps는 프로세스고 -a는 도커가 생성한 모든(all) 컨테이너, 프로세스 표시.
  - 예시에서 가져와 설치한 노드 인터랙티브 셸은 컨테이너에 의해 우리에게 노출되지 않는다. `docker run -it node`를 실행하는데 -it는 도커에게 컨테이너 내부에서 호스팅 머신으로 대화형 세션을 노출하고싶다고 알리는 것이다. 실행하면 인터랙티브 노드 터미널에 접속된다.
  - 또 docker run node로 각각 2번 실행했을 때 컨테이너 목록을 보면 같은 이미지로 만든 컨테이너가 2개 생성된 것을 볼 수 있다.

- 나의 이미지 구축

  - Dockerfile에 내 이미지를 구축할 때 도커에 사용할 명령을 적는다. `FROM node` 을 통해 다른 베이스 이미지에 내 이미지를 구축한다. 처음부터할 수 있지만 코드에 필요한 기타 도구 같은 운영체제 레이어가 필요하다. 이전에 node를 로컬로 받았기 때문에 도커에 같은 이름의 이미지가 존재해 찾을 수 있다.
  - `WORKDIR /app`: working directory 명시
  - `COPY . /` : 로컬에 있는 파일 중 이미지에 들어가야 할 것을 알려준다. 첫번째 .는 모든 파일 복사. 그 다음은 그 파일을 저장해야하는 이미지 내부 경로로 컨테이너 내부의 /app에 저장된다. (명시적으로 /app으로 작성해도 된다. 이 편이 보기 편하다.)
  - `RUN npm install`은 그 다음에 실행할 명령어로 도커 컨테이너 및 이미지 작업 디렉토리에서 실행된다. 디폴트는 컨테이너 파일 시스템의 루트 폴더다.
  - 마지막으로 서버 실행 명령인 RUN node server.js를 적을 수 있지만 틀리다. 이미지가 빌드될 때마다 실행되기 때문이다. 모든 것은 이미지 설정을 위한 도커 명령이어야 한다. 이미지 실행이 아니라 이미지 기반의 컨테이너를 실행하는 것이다. 하나의 이미지에서 여러 컨테이너를 시작하면 노드 서버도 여러개 시작되게 된다.
  - `CMD ["node", "server.js"]`: RUN과의 차이는 이미지가 생성될 때 실행되는게 아니라 컨테이너가 시작할 때 실행된다는 것이다. 명령어 작성 방식이 약간 달라 배열로 문자열을 넣어야 한다.
  - 이렇게까지 한 다음 실행하려고 하면 되지 않는다. node 서버는 80번으로 지정되어있지만 기본적으로 컨테이너는 격리되어 있어 자체 내부 네트워크를 가진다.
  - 따라서 마지막 명령 전에 `EXPOSE 80` 를 작성해 우리의 로컬 시스템에 특정 포트를 노출한다.

- 자체 이미지 기반으로 컨테이너 실행하기

  - `docker build .`: 같은 폴더 내의 도커파일을 빌드한다.
  - `docker run {id}`: 빌드 후 생성된 id를 넣는다. `id는 항상 전체를 복사할 필요없이 첫 몇개를 사용해도 된다. 같은 글자로 시작하는 id가 없으면 한 글자만 넣어도 된다. ` 그런데 실행해도 localhost에서 나타나지 않는다.
  - EXPOSE 80은 문서화 때문에 추가된 것이고 기능은 없다. 있으면 좋다.
  - 대신 실행 시 `docker run - p {localPort}:{exposePort} {id}`를 실행한다. -p는 publish로 도커에게 어떤 로컬 포트가 있는지 알린다. 액세스하려는 로컬포트를 지정하고 내부 도커 컨테이너 노출 포트를 넣는다.

- 코드 변경후 컨테이너를 재실행해도 변경사항이 반영되지 않음. 이미 코드를 app 폴더에 복사한 후 시작하는데 복사 시점에 소스 코드 스냅샷을 만든다. 코드를 편집하면 반영되지 않고 `새 이미지로 복사하기 위해 다시 빌드해야 한다.` 이미지 이름도 달라지게 된다.

- 이미지 빌드하거나 재빌드할 때 `변경된 부분 명령과 그 이후가 모두 재평가(뒤에 npm install이 있다면 전부 재설치)`된다는 것이다. 따라서 변경사항 없이 다시 빌드하면 캐시를 사용해 빠른속도로 완료된다. 모든 명령은 도커파일의 레이어를 나타낸다. `이미지는 다양한 레이어 기반으로 구성되며 읽기 전용이다. ` 

- 그 대신 최적화를 위해 RUN npm install 전에 `COPY package.json /app`을 사용하고 install 후에 `COPY . /app`을 실행하도록 위치를 변경한다. 소스 코드 복사전 install해서 재실행하지 않는다.

- `--help`를 추가해 사용가능한 명령어를 볼 수 있다.

- docker run은 새 컨테이너를 만들고 실행하는 것이므로 변경되지 않은 상태로 기존 컨테이너를 다시 시작하기 위해 `docker ps -a`로 중지된 컨테이너까지 확인한 후 `docker start {name or id}`로 시작한다. 터미널 프로세스가 즉시 완료되어 백그라운드로 실행 가능하다. 따라서 run과 다르게 터미널 상호작용이 가능하다.

  - attached모드: docker run으로 시작할 때 / 실행중인 컨테이너에 연결되어 있어 출력결과를 수신가능(콘솔 기록처럼). 포트 번호 뒤에 -d를 추가해 detached모드로 실행가능

  - detached모드: docker start로 시작할 때 / start로 시작할 때 -a를 붙여 바로 attached모드 가능

  - `docker attach {id or name}`: attach모드 실행

  - `docker logs {id or name}`: 컨테이너에 의해 출력된 로그 확인 가능. 또한 name 앞에 -f를 추가해 연결 프로세스를 가져 출력결과를 계속 볼 수 있다.

    이미 실행중인 컨테이너에 연결하기 위해서는 `docker attach {id or name}`

- 도커는 웹 서버에 대한 것만이 아니다. 파이썬 애플리케이션을 만들어 python run으로 실행하면 attached상태기 때문에 컨테이너에 의해 출력된 결과를 받을 수 있는데 입력이 불가능하다. --help에서 살펴보면 -i (input가능) -t(pseudo terminal 생성) 옵션을 볼 수 있다. `docker run -it {id}`로 실행하면 입력받는 터미널이 열린다. `-it는 -i -t도 된다.`다만 한번 입력하면 끝난다. 그 다음에 attached 옵션을 가진 start로 시작하면 이상하게 두 번 입력받아야하지만 한 번밖에 입력받지 못한다. 대신 start 옵션 중 출력을 수신하기 위해 -i가 존재한다. 여기서 -t는 다시 입력할 필요없는데 처음에 실행한 설정을 기억하고 있기 때문이다. `docker -a -i {id or name}` 

- 필요하지 않은 이미지와 컨테이너 삭제: `docker rm {id or name}`으로 가능하지만 실행중인 것은 stop후 진행가능. 여러 개를 삭제하고 싶다면 그냥 여러개 쓰면 됨. 필요하지 않을 때 바로 제거하기 위해 `docker images`로 만든 이미지를 본다. `docker rmi {image_id}`. 이미지는 중지된 컨테이너에 포함되었을 때만 삭제 가능하다. `컨테이너를 먼저 제거해야한다.` `docker image prune`은 사용되지 않는 모든 이미지를 제거한다.

- 컨테이너 중지할 때 자동 제거하기: `docker run --rm {id}`  컨테이너 노드 서버같은 것이 포함된 경우 코드가 변경될 때만 중지하는 경우가 많고 이는 이미지를 다시 빌드해야함을 의미한다. 

- 이미지는 크기가 크지만 컨테이너는 이미지 위에 명령 레이어가 추가된 얇은 부가 레이어다. 한 이미지기반으로 실행되는 여러 컨테이너는 이미지 내부 코드를 공유한다. 

- `docker image inspect {id}`를 사용하면 이미지 정보 출력. 생성시간, 관련 컨테이너, 노출 포트, 환경 변수, 디폴트 entry point, docker version등을 볼 수 있다. 레이어는 우리가 작성한 도커파일의 레이어뿐 아니라 다른 것들이 추가된다. 

- 컨테이너 살펴보거나, 추가, 추출하고싶다면? `docker cp {복사할파일} {name}:{복사될위치}`로 실행중인 컨테이너나 그 밖으로 파일, 폴더를 복사가능하다. 다만 우리가 복사되었는지 확인할 수 없기 때문에 테스트를 위해 파일 삭제 후 `docker cp {name}:{복사된위치} {복사될위치}`로 컨테이너에서 로컬로 복사해 확인 가능하다.

  - 보통 코드 수정 후 재빌드 해야하지만 단순히 복사해서 사용할 수 있다. 오류가 일어나기 쉬워 잘 사용은 안한다고 한다. 좋은 방법x

- 컨테이너 이름 지정: `docker run --name {지정할이름} {id}`

- 이미지 태그 지정: 태그는 이름(이미지의 리포지터리)과 태그가 존재하는데 둘은 :으로 구분된다. 컨셉상 이름은 여러 특정화된 이미지 그룹을 만들 수 있다. 태그는  옵션으로 그룹내에서 이미지의 특정화된 버전을 정의한다. 도커파일에서 FROM node:14를 사용해 14버전을 가져올 수 있다. docker hub에서 지원하는 많은 태그가 있다. `docker build -t {repository}:{tag} .`

- 이미지를 공유하기 위해 도커파일을 공유하거나 빌드된 이미지 자체를 공유할 수 있다. 도커파일로는 빌드를 시행하고 파일 구조가 필요하지만 이미지를 받으면 즉시 컨테이너 실행 가능하다.  따라서 `이미지 공유가 일반적`이다. 

- 이미지를 도커 허브 또는 개인 레지스트리에 업로드해 pull / push를 사용해 공유한다.

- 도커 허브에 push

  - docker login`을 통해 로그인`
  - `docker build -t {사용자id}/{repository}:{tag}`로 이미지를 만들거나 `docker tag {기존 respository}:{tag} {사용자id}/{repository}:{tag}`로 이미 있는 이미지를 복제해 도커 허브에서 생성한 레포지토리 이름과 같게 만든다. 
  - `docker push {사용자id}/{repository}:{tag}` 업로드할 때 우리가 도커허브의 node를 기반으로 만든 이미지라면 이를 제외한 추가 정보만 업로드하게 된다.

- 공유 이미지 pull: public 리포지토리라면 로그아웃해도 가능

  - `docker pull {사용자id}/{repository}`: 최신 소스 가져온다.
  - `docker run {사용자id}/{repository}`: 로컬에 없으면 자동으로 허브에 접속해서 자동 pull
  - docker run은 자동으로 전에 받은 이미지를 최신으로 업데이트 하지않으므로 pull을 내가 해야한다.



## 섹션 3: 데이터 관리 및 볼륨으로 작업하기

- 코드나 환경 설정을 위한 도구는 이미지에 더해져 한 번 이미지가 만들어지면 변경되지 않는다. read-only
- 임시 앱 데이터는 실행중인 컨테이너에서 만들어지는 것으로 컨테이너 안에 일시적으로 보관된다. read+write
  - 컨테이너 안에 저장된다는 것은 이미지 위에 도커에 의해 추가된 extra layer로 이미지, 이미지 파일 시스템을 인식하고 복사없이 파일 시스템을 미러링하는 로직이다. 근데 이건 부가적인거고 여기서 도커는 실제 read-write 액세스 권한을 가지고 파일 시스템을 조작할 수 있다. 이미지가 아닌 부가 레이어에서 변경하면 도커가 컨테이너 변경 사항 추적하고 이미지 파일 시스템을 가져와 최종 파일 시스템을 파생시켜 read-write 레이어에 저장된 변경 사항과 결합시킨다. 어쨌든 저장하라고 하면 read-write안이나 그 도움을 받아서 저장하는데 로컬이 아니라 컨테이너에 저장되는 것이다.
- 영구 애플리케이션 데이터는 컨테이너가 삭제되어도 존재해야 한다. 새 버전을 배포할 때 손실되면 안된다. read+write 컨테이너에 저장되지만 볼륨의 도움을 받는다. 도커 핵심개념
- Dockerfile은 로컬 폴더를 이미지에 복사하고 컨테이너는 그 이미지를 기반으로 해 로컬 폴더 자체 파일 시스템이 존재하므로 컨테이너에서 작업하며 생성한 파일은 로컬에서 보이지 않는다.
- docker run할 때 --rm옵션과 함께 실행하면 컨테이너 중지 후 다시 실행했을 때 생성해던 파일이 사라지지만 rm옵션 없이 한 경우에는 다시 실행하면 파일이 있다. 이런 문제를 위해 `볼륨`사용
- Volumes: 데이터를 유지하도록 돕는다. 호스트 머신(내 컴퓨터)의 폴더로 컨테이너, 이미지에 있지 않다. 도커 컨테이너내부 폴더에 매핑된다. 두 폴더의 변경 사항은 다른 폴더에 반영된다. 볼륨은 컨테이너 종료 후에도 유지된다. 컨테이너는 볼륨에 데이터를 읽고 쓸 수 있다. 
- Dockerfile에 `VOLUME [ "/app/feedback" ]`추가. WORKDIR이 /app, COPY . /app을 통해 전체 애플리케이션을 복사한 작업 디렉토리로 사용하고 있기 때문에 이게 내 컨테이너의 내부 경로다. 도커에서 관리하기 때문에 우리는 로컬에서 어디에 있는지 모른다. 추가 후 다시 빌드해서 실행하면 웹페이지가 정상적으로 작동하지 않는다. `docker logs  {container name}`으로 이유를 볼 수 있는데 node rename메서드에서 copyFile변경하고 어쩌고해서 고치고 다시 실행하면 volumes를 사용했어도 컨테이너 삭제 후 다시 run하면 파일이 사라져있다.
  - --rm 옵션을 제거하고 실행하면 익명 볼륨은 제거되지 않지만 컨테이너를 새로 시작하면 새로운 익명 볼륨이 연결되어 사용하지 않는게 쌓이게 된다. 이를 제거하기 위해 `docker volume rm {volume name}` 또는 `docker volume prune`을 사용한다.
- 도커의 외부 데이터 저장 매커니즘
  - Volumes: 두 경우 다 일부 폴더, 경로를 호스트 머신에 설정. `docker volume`명령어를 통해 접근. 
    - 익명의 볼륨(anonymous volumes): 바로 위에서 했던 방법. 이미지 기반 컨테이너에 데이터 추가. 위에서 적용했던 volume을 확인하기 위해 docker volume ls로 보면 이상한 이름을 가진 볼륨이 보인다. docker stop 후에 다시 확인하면 사라져있다. 도커가 관리하는데 익명이기 때문에 컨테이너가 존재할 때만 존재한다. `하나의 컨테이너에 밀접하게 연결`
    - named volumes: 컨테이너가 제거된 후에도 하드 드라이브에 유지된다. 새 컨테이너를 시작하면 볼륨, 폴더가 복구. / 따라서 영구적이어야하거나 직접 편집하거나 볼 필요없는 중요 데이터에 적합.
  - Bind Mounts: 우리는 코드를 수정해도 이미지를 다시 빌드하지 않는 이상 바로 반영되지 않는데 도커 이미지 생성시 폴더 스냅샷을 저장하기 때문에 미래 변경사항은 반영되지 않는다. 바인드 마운트의 볼륨과의 차이는 볼륨은 위치를 모르지만 바인드 마운트는 안다. 우리가 호스트 머신 경로를 설정한다. 소스 코드를 바인드 마운트에 넣으면 컨테이너가 인식해 스냅샷에서 코드를 복사하지 않고 바인드 마운트에서 복사한다. `영구적이고 편집가능한` 데이터에 적합. 이미지가 아닌 실행 컨테이너에서 적용되기 때문에 dockerfile에서 설정하지 않는다.
- 도커 내부에 이름있는 볼륨을 만들 수 없기 때문에 dockerfile의 volume을 제거하고 컨테이너 실행 시 명명된 볼륨을 생성한다. `docker run -d -p {localPort}:{exposePort} --rm --name {container name} -v {볼륨 이름}:{컨테이너 파일 시스템 내부 경로} {iamge name}` / 다른건 그냥 옵션이고 여기서는 -v부터 추가되었는데 컨테이너 파일 시스템 내부 경로는 위에서 입력했던 /app/feedback이다. docker stop으로 종료될 때 삭제되게 한 후 다시 똑같은 run명령어로 `동일한 볼륨 이름`을 지정해 실행하면 이전의 데이터가 남아있다.
- 바인드 마운트 적용: 이전에 명명 볼륨을 사용하기 위해 실행했던 명령어에서 볼륨 관련 경로 설정 뒤에 다시 `-v "{호스트 머신 상의 절대경로}:/{컨테이너 내부에 매핑하려는 폴더(여기서는 /app)}"`를 사용해 두번째 볼륨을 추가한다. 모든 코드가 있는 절대 경로가 /app에 매핑된다. 경로에 특수문자, 공백이 있을 경우를 대비해 "으로 감쌌다.
  - 이 때 도커가 액세스할 수 있는지 확인해야한다. preferences - resources - file sharing에 공유하는 폴더가 리스트되어 있는지 확인한다. (그 상위폴더가) / 근데 나처럼 wsl로 실행중이면 이 옵션이 없다. / 또 docker toolbox로 실행중이라면 디폴트로 사용자 폴더가 공유중이며 자세한 설정은 따로...
  - 명령어를 실행하면 컨테이너 내부 app 폴더에 명명된 볼륨으로 마운트된다. 하지만 종속성을 찾지 못한채로 종료되는 오류가 발생한다. 현재 경로로 지정한 곳에 있는 모든 파일을 app폴더에 바인딩하는데 dockerfile에서 package.json을 복사하고 npm install 을 통해 node_modules를 생성하지만 그 후에 run하면서 현재 폴더를 다시 app폴더에 덮어쓰면서 사라진다. 그러면서 현재 폴더는 node_modules가 존재하지 않기 때문에 에러가 발생한 것이다.
  - 이 때 세번째 볼륨, 익명 볼륨을 추가한다. 두번째 볼륨 설정 뒤에 `-v /app/node_modules`를 추가한다. 도커는 컨테이너에 설정하는 볼륨을 평가하며 충돌이 있으면 더 길고 구체적인걸 우선시한다.  바인딩된 app과 바인딩된 app/node_modules가 있기 때문에 더 긴걸 택한다. 그래서 node_moduls는 살아남아 바인드 마운트 내용물에 덮어씌워진다.
  - html 변경사항은 바로 반영되는 것과 달리 node.js의 파일은 노드 런타임에 의해 실행된다. 서버 변경사항은 재시작해 반영해야 하는데 컨테이너 전부가 아닌 웹 서버만 재시작하면 된다. docker stop한 후 재시작하면 되지만 번거로워 node js용 확장 패키지를 사용한다. package.json에서 devDependencies에 nodemon을 추가하면 nodemon을 사용해 서버가 시작되어 변경사항을 감지한다. 또 scripts를 추가해 start로 nodemon server.js를 시작하게 하고 dockerfile에서도 node server.js가 아닌 npm start로 변경한다.
  - 그런데 wsl2를 사용중이라 리눅스 파일시스템에 내가 직접 저장해야 한다. linux 파일 시스템에 마운트해 작업하면 프로젝트 변경사항이 도커 컨테이너로 전파된다.
    - 첫 번째 방법으로 nodemon이 스스로를 직접 감시하도록 scripts를 nodemon -L server.js로 변경한다. quick and dirty 방법이라고 한다. 리액트도 이런 모드가 있는거같다.
    - 아니면 윈도우 빌드 20211로 업데이트하면 wsl2에서 리눅스 파일 시스템에 엑세스 할 수 있게 된다. 그리고 마이크로소프트에서 시키는대로 한다.
    - 3번째가 베스트라는데 embrace wsl2라는 방법으로 wsl2를 디폴트 터미널로 사용한다. 그래서 모든 작업을 wsl2로 한다. 이 말대로 그냥 vs code를 wsl 확장 프로그램을 깔아서 원격개발하니까 된다.
- 읽기전용볼륨: 컨테이너는 로컬 파일을 변경할 수 없다. 우리가 파일을 변경한느 곳은 호스트 머신 파일 시스템이지 컨테이너 내부가 아니다. 하려는 바를 명확히 하기 위해 바인드 마운트를 읽기 전용 볼륨으로 전환한다. 
  - 바인드 마운트에서 컨테이너 내부에 매핑하려는 폴더뒤에 `:ro`(read-only)를 붙여 읽기전용으로 지정하면 도커는 그 폴더와 하위 폴더에 쓸 수 없게된다. 다만 폴더 전체가 바인드 마운트 되어있고  server.js코드에서 실행결과가 temp폴더와 feedback폴더에 저장되도록 되어있다. feedback폴더는 이름있는 볼륨으로 지정된 상태기 때문에 구체적인 하위볼륨을 지정해 메인볼륨보다 우선하게 한다.  `-v /app/temp`로 temp를 익명볼륨으로 추가한다.
- `docker volume create {name}`로 생성가능. 
- `docker volume inspect {name}`으로 생성시간, 마운트 포인트(생성된 곳. 작은 가상 머신 내부에 존재), 이름 등을 볼 수 있다.
- `docker volume rm {name}`으로 제거
- `docker volume prune`으로 사용하지 않는 볼륨들 제거
- COPY와 바인드 마운트: 어차피 전체 폴더를 바인드 마운트하는데 왜 COPY할까? 주석하고 실행해도 잘된다. 하지만 바인드 마운트는 개발 중에 사용하는 명령으로 변경사항을 즉시 반영하므로 개발이 끝난 후 프로덕션(실제 사용)에서는 스냅샷 이미지가 필요하므로 이 때 copy를 사용한다. 스냅샷 컨테이너를 생성하는 옵션이 반드시 필요하다.
- .gitignore처럼 .dockerignore사용: COPY .으로 모든 걸 복사할 때 만약 로컬에 node_modules가 존재한다면 npm install로 복사된게 덮어씌워진다. 하지만 최신 버전을 유지하기 위해 새로 설치해야하므로 제외할 수 있다.
- 환경 변수와 .env파일: 도커는 빌드 타임 인수, 런타임 환경 변수 지원. 인수 사용하면 dockerfile에서 특정 명령으로 다른 값을 추출하는데 사용할 수 있는 변수를 설정할 수 있다. 빌드 실행시  --build-arg와 제공되는 인수 기반. 환경변수는 도커파일 내부에서 사용하는데 인수는 실행중인 전체 애플리케이션 코드 내에서 사용 가능. 도커 파일내에서 env옵션을 사용해 환경 변수 존재를 도커에게 알린다. run에서 --env를 통해 정확한 값을 알린다. 
  - 환경 변수에 들어갈 데이터를 별도의 파일로 이동하지 않고 Dockerfile에 포함시키면 이미지에 포함되어 이미지 history를 볼 때 보여진다. 따라서 중요한 개인정보는 꼭 별도 파일로 분리하고 소스 컨트롤 저장소에 올리지 말자.
  - Dockerfile에 넣기: ENV PORT 80 / EXPOSE $PORT js에서는 process.env.PORT처럼 사용하지만 Dockerfile에서는 $뒤에 넣는다.
  - run할 때: 포트뒤에 `--env PORT:8000`처럼 적는다. `-e`만 써도 되며 여러개 사용할 경우 볼륨처럼 나열하면 된다.
  - .env 파일: PORT=8000 .env파일을 만들고 run할 때 `--env-file ./.env` 로 파일을 사용한다고 하고 위치를 명시한다.

- 빌드 타임 인수: 도커파일에 다양한 값을 플러그인 하거나 하드코딩하지 않고 이미지를 빌드할 때 다른 값을 플러그인(끼워 넣기) 할 수 있다.
  - dockerfile에 여전히 PORT 80이라고 되어있는데 이를 빌드 타임 인수로 변경하면 다른 디폴트 값으로 여러 이미지를 빌드할 수 있다. COPY 뒷부분에 `ARG DEFAULT_PORT=80`을 지정한다. ARG역시 도커에 한 레이어를 추가하며 변경되면 그 뒷부분을 다시 실행하기 때문에 npm install을 다시 하지 않기 위해서다. 이 값은 JS에서 사용할 수 없고 도커파일에서만 사용가능하며 모든 명령어에 사용할 수 있는건 아니다.(CMD 안됨) 컨테이너 시작될 때 실행되는 `런타임 명령어`기 때문이다. 그리고 일반적으로 이미지를 하나 만든 후 개발용 이미지를 만들기 위해 build명령어에서 . 앞에 `--build-arg DEFAULT_VALUE=8000`을 적으면 인수를 변경해 빌드된다.



## 섹션 4: 네트워킹: (교차) 컨테이너 통신

- 컨테이너 애플리케이션이 API로 HTTP 요청 보내는 경우: 기본적으로 월드 와이드 웹 요청 가능
- 컨테이너 애플리케이션이 호스트 머신의 서비스와 통신하는 경우. 도커 없이도 컴퓨터에서 실행되는 데이터베이스같은 것.
  - 도커가 이해할 수 있도록 localhost를 특수 도메인인 `host.docker.internal`로 변경. 도커 내부에서 호스트 머신의 ip주소로 변환. 현재 실습상 mongodb요청이지만 `http요청`에도 사용가능.
- 컨테이너 애플리케이션이 다른 컨테이너의 데이터베이스와 통신하는 경우
  - 현재 실습상 사용하는 mongodb는 docker hub에 이미지가 있기 때문에 Dockerfile건드릴 필요 없이 docker run mongo로 실행하면 현재 실행중인 컨테이너 목록이 계속 보이므로 `docker run -d --name mongodb mongo`로 detached 모드로 실행한다.
  - mongodb image를 만든 후 `docker container inspect mongodb`를 실행하면 ip address를 알 수 있다. 앞서 host.docker.internal을 ip address로 변경하면 된다.
- 엔드포인트: 요청 받아 응답을 제공하는 서비스를 사용할 수 있는 지점
- **Docker Networks**
  - docker run할 때 `--network` 옵션을 사용해 컨테이너를 하나의 동일한 네트워크에 밀어넣을 수 있다. inspect로 ip를 확인해 사용할 필요없이 자동으로 수행할 수 있다.
  - 그러나 볼륨과 달리 network는 run에 옵션으로 넣어도 자동으로 생성해주지 않기 때문에 직접 컨테이너를 만들어야 한다.
  - `docker network create {network name}`으로 도커 내부 네트워크를 만든다. 
  - 그 다음 mongodb 컨테이너와 node 컨테이너를 run하고 기존 ip address가 적혀있던 곳에는 네트워크 이름을 써서 연결하면 된다.
  - 한 네트워크에서 DB 컨테이너를 실행할 때 -p 옵션을 사용해 포트를 publish(게시)할 필요 없다. `컨테이너 간 연결이 있다면 포트를 게시할 필요없다.` 내부에서 모든 컨테이너가 자유롭게 통신 가능하다. 로컬 호스트 머신이나 외부에서  컨테이너에 연결할 때 필요하다.
  - 실제 네트워크 동작에 영향을 미치는 `드라이버`를 지원한다. 디폴트 드라이버인 `bridge`는 동일한 네트워크에 있을 대 이름으로 서로 찾을 수 있게 한다. 드라이버는 네트워크 생성 시 --driver 옵션을 추가하고 드라이버 이름을 적어 설정한다. 그 외 host, overlay, macvlan, none과 써드파티 플러그인이 있다.



## 섹션 5: Docker로 다중 컨테이너 애플리케이션 구축하기

- react앱을 실행할 때 `-it`옵션 필요. react프로젝트는 -it옵션으로 인터랙티브 모드로 실행해야 한다. 아니면 서버가 즉시 중지된다. 
- 한 네트워크에 컨테이너들이 존재할 때 backend에서 db에 연결할 때 포트 노출은 db이름을 필요로하고 프론트엔드에서는 localhost를 백엔드 이름으로 변경하면 될 줄 알았지만 안된다.
- 프론트엔드에서는 로컬 호스트 머신에서 상호작용하기 위해 여전히 포트를 게시한다. 그래서 브라우저에서 테스트 할 수 있으면서 네트워크에 들어있게 된다. 백엔드는 런타임에 의해 컨테이너에서 직접 실행되지만 `프론트엔드는 컨테이너 내부가 아닌 브라우저에서 실행`된다. 따라서 브라우저는 한 네트워크 안에 있는게 아니라 프론트엔드에 지정한 백엔드 이름을 알 수 없다.
- 따라서 docker run으로 백엔드를 실행할 때도 network뿐 아니라 프론트엔드 애플리케이션과 통신할 수 있게 포트를 여전히 노출해야 한다.
- 현재 DB컨테이너를 중지하면 데이터가 모두 날아간다. 그래서 그 데이터를 분리해 하드 드라이브에 저장해 여전히 살아남게 해야한다. 명명된 볼륨을 사용해서 우리 컴퓨터 어딘가에 존재하면 데이터를 불러오고 아니면 폴더를 생성해 저장하도록 한다. mongodb의 경우 컨테이너 내부 데이터베이스 파일 저장하는 곳은 /data/db다. 따라서 `-v {volume name}:/data/db`를 추가했다. 
- **보안 & 데이터 액세스 방지**
  - 컨테이너를 만들 때 MONGO_INITDB_ROOT_USERNAME, MONGO_INITDB_ROOT_PASSWORD를 추가해 엑세스할 때 사용하게 한다. 그리고 연결되는 uri에서 호스트 주소앞에 `username:password@호스트주소(db 이름)`를 추가한다. 그리고 연결문자열 끝에 `?authSoruce=admin`을 추가한다.
- backend 도커파일에 ENV 환경변수를 추가한 후 백틱안에 ${proces..env.}와 함께 사용하면된다. 또 run할 때 -e로 재정의해서 사용할 수 있다.
- 로컬에서 실행해 node_modules가 존재할 경우 frontend docker 실행 최적화를 위해 .dockerignore에 node_modules를 추가하면 좋다. 그 외 .git, Dockerfile



### 섹션 6: Docker Compose: 우아한 다중 컨테이너 오케스트레이션

- 지금까지 네트워크를 만들고 상당히 긴 명령어로 백엔드, 프론트엔드, DB 이미지를 빌드하고 컨테이너를 시작해야 했다. 이런 다중 컨테이너 설정을 쉽게 관리할 수 있는 설정 프로세스 자동화를 위해 `도커 컴포즈`를 제공하고 있다.
- Docker-Compose: build, run 명령을 대체할 수 있는 도구로 다수의 명령을 하나의 구성파일로 가진다. 이를 통해 모든 이미지를 빌드하고(변경됐을 때만 리빌드) 컨테이너를 시작하는 `오케스트레이션(Orchestration, 자동화된 설정) `명령 셋이다.
  - 도커파일을 대체하지 않고 함께 동작한다.
  - 이미지, 컨테이너를 대체하지 않는다.
  - 다수의 호스트에서 다중 컨테이너를 관리하는데는 적합하지 않다. 하나의 호스트에서 다중 컨테이너를 관리하는데 좋다.
  - 도커 컴포즈 파일에서 정의해야하는 가장 중요한 핵심 항목은 `서비스`다. 이는 `다중 컨테이너 애플리케이션을 구성하는 컨테이너`다. 게시할 포트 정의, 환경 변수 정의, 볼륨, 네트워크 정의도 가능하다. 터미널에서 도커 명령으로 할 수 있는 모든 것을 컴포즈에서 할 수 있다.
- 프로젝트 루트에 docker-compose.yaml을 만든다. yaml은 들여쓰기를 사용해 구성 옵션 간 종속성을 표현하는 특정 텍스트 포맷이다. 
  - [compose-file](https://docs.docker.com/compose/compose-file/): 사용 가능한 옵션을 볼 수 있다.
  - detached모드와 중단하면 바로 제거되는 rm이 기본이다.
  - `version`:은 도커 컴포즈 버전. 사용할 수 있는 구문을 계속 개발 중이라 시간이 지나면 변경될 수 있다.
  - `services`: 하위에 각 컨테이너가 들어간다.
    - mongodb에서 `image`는 로컬, 도커 허브 또는 다른 저장소에 존재하는 이미지 url / `volumes`는 명명 볼륨일 경우 그대로 이름과 위치 표기. 또한 절대경로로 표시하던 것을 상대경로로 표시 가능하다. / `enviornment`는 다른 변수들처럼 콜론을 사용하거나 -(bar)와 =를 사용해서 정의해도 된다. / .env파일을 따로 만든후 `env_file`로 가져와 `- 위치`를 쓴다.
    - backend에서 `build`는 도커파일이 위치한 곳을 필요로한다. 그 위치에서 빌드되므로 복사할 폴더가 포함되는 폴더로 설정해야하며 아닐경우 상위폴더를 지정해야 하지만 헷갈린다. 또는 build에서 하위에 `context`를 설정하고 위치를 지정한 후 `dockerfile`에서 파일 이름을 알려주기도 한다.(파일 이름 다를 때) 그리고 `args`로 arguments를 넘겨준다. / `ports`는 하위에 `-80:80`처럼 사용 / `depends_on`은 run에 없는 옵션으로 여러 컨테이너를 동시에 만들기 때문에 필요해졌다. backend는 db가 만들어진 것에 의존하고 있기 때문에 의존성을 지정해줘야 한다.
    - frontend에서는 -it옵션대신 `stdin_open: true`와 `tty: true`를 적어준다.
  - 도커가 이 컴포즈 파일에 특정된 모든 서비스에 대해 새 환경을 자동으로 생성하고 모든 서비스를 즉시 그 네트워크에 추가한다. networks를 추가해 특정 네트워크를 사용해도 된다. 그러면 디폴트 네트워크에 특정 네트워크가 추가된다.
  - 명명된 볼륨일 경우 version, services와 같은 구간에 `volumes:` 를 적고 하위에 위에 나온 명명 볼륨의 이름을 적어줘야 한다. 명명 볼륨을 인식하기 위해서이며 다른 컨테이너가 동일한 볼륨 이름과 동일 위치를 사용할 경우 서로 공유해서 쓸 수 있다.
- `docker-compose up` 명령어를 사용하면 이미지가 빌드되고 컨테이너가 시작한다. up대신 build를 쓰면 빌드만된다.
  - -d: detached모드로 실행
  - --build: 이미지 강제 리빌드
- `docker-compose down`으로 모든 컨테이너 삭제, 디폴트 네트워크 종료. 그런데 볼륨은 삭제되지 않으므로 -v추가 필요하다.(보통 데이터는 남겨두길 원하니까)
- 우리가 컴포즈에 정의한 이름과 다른 이름이 보여지는데 컨테이너 이름의 일부로 선택되며 프로젝트 폴더_서비스 이름 등으로 지정되며 원하는 컨테이너 이름을 위해서는 `container_name`을 사용하면 된다.



## 섹션 7: "유틸리티 컨테이너"로 작업하기 & 컨테이너에서 명령 실행하기

- 유틸리티 컨테이너: 공식 용어x / 애플리케이션 컨테이너는 환경과 코드가 함께 들어있다. 유틸리티 컨테이너는 특정 환경만 포함한다. 실행할 때 애플리케이션을 시작하지 않고 특정 작업을 실행하기 위해 다른 부가 명령어를 사용한다.
  - 애플리케이션을 설정하기 위해 처음 npm init을 사용해 package.json을 직접 입력하지 않고 만들어야 하지만 node가 설치되어있어야만 한다. 그러니까 컨테이너에 애플리케이션을 넣을 수는 있지만 초반 설정을 위해서는 여전히 호스트 머신에 툴이 설치되어있어야 한다.
- docker에서 node를 실행하려면 `docker run -it node`로 인터랙티브 모드로 실행해서 명령어를 사용하면 된다. 그리고 detached모드로 실행했을 때 `docker exec -it {container name} {node command}`를 사용한다. `exec`는 dockerfile에 정의된 기본 명령 외에 추가명령을 실행할 수 있게 해준다. 이런 방법을 사용해 시스템에 설치되지 않은 툴을 사용해 프로젝트를 시작한다.
  - `docker run -it node {node command}`는 노드 실행파일 자체인 default 명령을 오버라이드해서 바로 실행시킨다.
  - 다만 위의 두 방법의 경우 컨테이너 안에 프로젝트가 존재해 로컬 호스트 머신에서 접근할 수 없다.
- Dockerfile에서 node버전과 workdir을 지정한 후 npm init명령어도 사용할 수 있지만 이미지를 제한하지 않고 명령을 사용할 수 있게 한다. 그리고 로컬 호스트 머신에 바인드 마운트를 사용해 로컬에 작업 디렉토리를 바인딩한다. `docker run -it -v {local path}:{container workdir path} {container name} npm init` 이 명령어를 실행하면 package.json이 로컬에 생성된게 보인다.
  - npm init대신 npm install express 등을 사용해 다른 명령어도 실행.
  - 그리고 dockerfile에서 CMD ['command']가 있다면 이를 대체해서 명령어가 실행되는 것이지만 `ENTRYPOINT ['command']`가 있으면 터미널에서 사용하는 명령어가 엔트리포인트 명령어 뒤에 덧붙여진다. 따라서 npm이 입력되어있다면 install만 사용해 실행가능하다.

- docker-compose.yaml을 사용해 긴 명령어를 줄일 수 있다. 여기서도 이미 생성되어 실행중인 컨테이너에 명령을 추가하기 위해 `docker-compose exec`, `docker-compose run`을 사용할 수 있다. 그래서 `docker-compose run --rm {service name} {command / ENTRYPOINT뒤에 올 명령어}`로 실행한다.



### 섹션 8: 더 복잡한 설정: Laravel & PHP 도커화 프로젝트

- 호스트 머신-소스 코드 폴더 / PHP Interpreter / Nginx Web Server / MySQL DB / Utilities(Composer-npm같은거, Laravel Artisan-db migration용, npm-larabel frontend용)
- niginx

```yaml
version: '3.8'

services:
  server:
    image: 'nginx:stable-alpine'
    ports:
      - '8000:80'
    volumes:
      - ./src:/var/www/html # 서버에서 php요청에 대해 알고있어야해서???
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - php
      - mysql
```

- php

```dockerfile
FROM php:8-fpm-alpine  # nginx 구성을 위해

WORKDIR /var/www/html  # 웹 서버의 꽤 표준적인 폴더라고 함(?)

RUN docker-php-ext-install pdo pdo_mysql  # 부가 종속성 설치 도구
```

```yaml
  php:
    build:   # Dockerfile이 아닌 다른 이름이라서 context사용
      context: ./dockerfiles
      dockerfile: php.dockerfile
    volumes:  
      - ./src:/var/www/html:delegated # 컨테이너가 일부 데이터 기록할 때 배치로 처리해 성능 향상(안정성 약간 저하 속도 향상)
      
    # nginx.conf에서 php요청을 처리할 때 3000으로 보내게 되어있는데 php의 기본 도커 이미지는 내부에서 9000을 노출하고 있다. 따라서 3000을 9000으로 매핑한다. 그래서 굳이 이렇게 하지말고 nginx.conf에서 9000으로 변경한다. 로컬호스트를 거치지 않고 컨테이너간 직접 통신하니까??
    # ports: 
      # - '3000:9000'
```

- artisan, npm

```yaml
  artisan:
    build: 
      context: ./dockerfiles
      dockerfile: php.dockerfile
    volumes:
      - ./src:/var/www/html
    entrypoint: ["php", "/var/www/html/artisan"]  # dockerfile로 분리하지 않고 사용
  npm:
    image: node:14
    working_dir: /var/www/html  # dockerfile로 분리하지 않고 사용
    entrypoint: ["npm"]
    volumes:
      - ./src:/var/www/html
```

- `docker-copmose up -d --build {service name}`으로 특정 서비스만 실행 가능. 또 --build를 사용해 dockerfile이 변경된 것 반영

- 배포할 계획이라면 바인드 마운트는 X. 로컬 호스트 머신의 폴더이기 때문에 다른 서버에 이 컨테이너가 들어가면 폴더 구조가 존재하지 않는다. 거기서 다시 폴더 시스템을 만드는 것은 컨테이너의 사상이 아니다. `컨테이너에 필요한 모든 것이 컨테이너 내부에 있어야 한다.`  대신 도커파일에 소스 코드, nginx구성을 이미지로 복사해서 기존 코드 스냅샷 복사하고 빌드할 때 이미지 내에 구성을 갖게 한다. 이미지 배포할 때 필요한 소스 코드, 구성이 이미 포함되어 있다. 그래서 nginx.dockerfile을 따로 만든다.

```dockerfile
FROM nginx:stable-alpine

WORKDIR /etc/nginx/conf.d

COPY nginx/nginx.conf .

RUN mv nginx.conf default.conf # nginx.conf 파일은 컨테이너 내부에서 이름이 다르다. 

WORKDIR /var/www/html

COPY src .
```



docker-compose.yaml

- 지금까지 context가 ./dockerfiles고 dockerfile에 파일명만 지정했지만 사실 context는 `dockerfile이 빌드될 폴더`를 설정한다. 로컬에서 dockerfiles와 같은 위치에 nginx, src폴더가 위치하기 때문에 context를 dockerfiles로 지정하면 이미지 빌드가 실패하게 된다. 또 직접 copy했기 때문에 기존에 존재하던 volumes를 제거했다.

```yaml
  server:
    build:
      context: . 
      dockerfile: dockerfiles/nginx.dockerfile
    ports:
      - '8000:80'
    depends_on:
      - php
      - mysql
```

- 또 php.dockerfile 역시 copy가 필요하다. 그런데 소스 코드를 작업 디렉터리에 복사하면서 이미지가 컨테이너 읽기, 쓰기를 제한해 권한을 변경하는게 필요하다. php 이미지의 디폴트 유저(???)

```dockerfile
FROM php:8-fpm-alpine

WORKDIR /var/www/html

COPY src .

RUN docker-php-ext-install pdo pdo_mysql

RUN chown  -R www-data:www-data /var/www/html
```



### 섹션 9: Docker 컨테이너 배포하기

- 개발 중엔 바인드 마운트를 많이 사용하지만 배포할 때는 사용하면 안된다. 
- 어떤 컨테이너화된 앱들은 개발 후 배포하기 전 코드 변환 & 최적화되는 빌드 단계가 필요할 수 있다.
- 다중 컨테이너 프로젝트가 다수의 서버 / 호스트 머신에 분할되어야 할 수 있다. 
- 배포 플랫폼을 덜 통제하는 더 쉬운 솔루션을 찾을 수 있는 부분이 있다.(control and responsibility trade-offs) / 첫 번째 실습이 완전 통제형
- EC2: 클라우드에 있는 컴퓨터에 연결에 원하는 소프트웨어 설치. 여기서는 도커를 설치해 사용한다.
  1. EC2 인스턴스 생성해서 시작. VPC(Virtual Public Cloud, 가상 공개 클라우드)와 보안 그룹 생성. 인스턴스 액세스를 통제하기 위해서
  2. 필요한 포트를 www에 노출하기 위해 보안 그룹 생성. ec2인스턴스에 들어오는 트래픽을 가지도록.
  3. secure shell을 의미하는 ssh를 통해 이 인스턴스에 연결. 리모트 머신에 연결하기 위한 접근 방식으로 터미널 기반의 접근 방식. 리모트 컴퓨터에서 명령 실행 가능. 
- 개발중에는 컨테이너, 런타임 환경을 캡슐화해야하는데 코드는 꼭 그럴 필요 없다. 코드는 컨테이너 외부에서 제공된다. 바인드 마운트의 도움으로 로컬 프로젝트를 사용중인 컨테이너에 제공 가능하다. 즉각적인 업데이트가 가능하고 이미지를 리빌드해 재시작할 필요 없다.
- 배포중에는 컨테이너가 독립적으로 동작해 리모트 머신의 주변 설정에 의존하지 않는다. `이미지와 컨테이너는 단 하나의 소스여야 한다.` 소스 코드를 리모트 머신의 특정 폴더로 이동시킬 필요 없다. 호스팅 머신의 컨테이너 주변에는 아무것도 없어야 한다. 그래서 바인드 마운트가 아닌 COPY를 사용한다. 이미지 빌드할 때 소스 코드를 이미지에 복사해 빌드된 이미지에는 소스 코드와 애플리케이션 환경이 존재한다. 
  - 지금까지 바인드마운트 관련을 dockerfile이 아닌 run 명령어 실행할 때 직접 사용해 개발과 배포에 동일한 dockerfile을 사용할 수 있게 했다.
  - docker-compose에는 작성했기 때문에 추후 다른 방법을 알아본다.
- EC2: 클라우드 호스팅 프로바이더 / Your Own Remote Machines
  - AMI(Amazon Machine Image): 인스턴스가 구동되도록 구성할 수 있는 마법사 시작해서 클라우드 서버 구동. `Amazon Linux 2 AMI, 64bit(x86)`을 기본적으로 서버에 설치할 운영 체제로 선택. 인스턴스는 `t2.micro` 
  - 새 키 페어 생성. SSH를 통해 인스턴스에 연결해 인스턴스에서 명령을 실행하기 위한 파일이 되는 key pair가 필요하다. 그러면 키 파일이 다운로드되는데 연결하기 위해서 필요하며 오직 한 번만 받을 수 있다. 분실하면 새 인스턴스를 시작해야 한다. 받은 파일을 프로젝트 폴더에 넣는다.
  - 인스턴스를 생성해 실행되면 SSH(Secure Shell)로 연결한다. cmd를 사용해 로컬 머신에서 리모트로 연결하기 위한 프로토콜.  리눅스, 맥은 일반 터미널에 내장되어 편하게 사용 가능하다. windows는 wsl2를 설정해야 한다. 또는 putty(ssh client)를 다운받아 사용한다. 
  - 터미널창에서 퍼블릭 dns를 사용해 인스턴스에 연결하면 명령을 리모트 머신에서 실행할 수 있다. 그러고나면 도커를 설치하고 리모트 머신에 로컬에서 빌드한 이미지를 가져오면 된다.  그리고 컨테이너 실행 - 보안그룹 구성
  - `sudo yum update -y`: 리모트 머신 필수 패키지 업데이트
  - `sudo amazon-linux-extras install docker`: amazon linux를 선택해서 쉽게 docker 설치 
  - `sudo service docker start`: 도커 시작
  - 로컬 이미지를 리모트 머신으로 가져오기 위해 소스코드(Dockerfile을 포함한 프로젝트 폴더 모든 항목을 리모트 머신에 복사해서 이미지 구축) 배포 => 불필요하게 복잡한 것 많음 / 로컬 머신에서 미리 이미지 빌드해 Docker Hub에 푸시해서 리모트 머신에 배포
  - 2번 방법을 위해 .dockerignore에 *.pem, node_modules, Dockerfile등을 등록하고 새로운 터미널에서 빌드한다. 그리고 `docker tag node-dep-example-1 {docker hub id}/node-example-1`로 태그를 등록하고 새로 만들어진 이미지를 `docker push {docker hub id}/node-example-1`
  - 다시 ssh 터미널로 돌아와 sudo docker run으로 앞서 올린 이미지를 실행할 수 있다. 보안 상 sudo말고 다른 방법이 있지만 지금은 저렇게 하기로 한다.
  - 그리고 인스턴스의 퍼블릭 ip로 접속하면 연결이 끊어져있어 ssh를 사용하는 사용자 외에는 접속 불가능하다. 연결된 보안 그룹은 기본적으로 인스턴스에서 허용되는 트래픽을 제어한다. 그 중 아웃바운드는 다른 곳에 있는 인스턴스 대기열로부터 허용되는 트래픽을 제어한다. 현재 모두 허용되고 있어 터미널에서 docker run이 가능했던 것이다. 그런데 인바운드는 ssh포트인 22만 허용되고 있다. 소스 0.0.0.0/0은 전세계 허용이므로 전세계에서 포트 22를 통해 이 인스턴스에 연결할 수 있다. 따라서 앞서 받은 키 파일로 자신을 식별하는 것이다. 아니면 소스를 특정 로컬 ip로 한정할 수 있다. http트래픽이 인스턴스에 접근할 수 있도록 인바운드 규칙에 http를 추가한다.
  - 소스코드를 편집한 내용을 반영하고 싶다면 이미지를 다시 빌드 - 푸시 - ssh에서 run하면 된다. 하지만 도커가 이미지를 실행할 때 로컬에 있는지 확인하고 사용해서 최신 리모트 이미지를 확인하지 않는다. 따라서 run 대신 `docker pull`로 가져와서 다시 run한다.
  - 현재 접근 방식은 수동으로 생성-연결-도커설치로 모든 게 수동으로 진행된다. 이런 방법을 사용할 때는 리모트 머신이 완전히 우리 소유물로 전부, 특히 보안에 책임져야 한다. 트래픽이 많이 발생하면 교체해야 하고 소프트웨어와 운영체제가 최신 상태여야 하며 네트워크와 보안 그룹, 방화벽을 관리해야 한다. 대신 배포작업절차를 통해 자동으로 이미지를 리모트로 이동시키고 운영체제가 업데이트 되는 관리되는 접근 방식을 권장한다. 
- ECS: Elastic Container Service / 관리형 리모트 머신
  - 써드파티 서비스에서 생성 관리, 업데이트, 모니터링, 스케일링을 처리해줘 단순화해준다. 
  - 도커를 사용하지 않고 클라우드 프로바이더가 제공하는 서비스를 사용하므로 컨테이너 배포, 실행도 도커 명령으로 수행되지 않는다.
  - 컨테이너 편집창에서 docker run을 실행할 때 사용하는 옵션과 관련한 요소들을 편집할 수 있다. 이미지는 dockerhub에서 기본적으로 찾는데 그 외 레지스트리를 사용할 경우 호스팅 도메인과 저장소 이름을 넣어야 한다. 포트에서는 컨테이너 내부와 외부가 항상 동일하기 때문에 하나만 입력하면 된다. `상태확인`은 aws가 특정 명령을 실행해 컨테이너가 성공적으로 실행 중인지 확인하는 설정이며 `환경`은 이 컨테이너에 실행되어야 하는 디폴트 entrypoint또는 명령을 오버라이드 할 수 있다.(working dir등 도커파일 관련 내용들) `스토리지 및 로깅`에서 로그 구성에 Auto-configure CloudWatch Logs를 사용하면 aws의 cloud watch서비스를 활용해 컨테이너에서 생성된 로그를 자동으로 관리하고 저장한다.
  - 작업 정의(Task definition)는 애플리케이션의 블루프린트다. 태스크에는 둘 이상의 컨테이너가 포함될 수 있다. `하나 이상의 컨테이너를 실행하는 하나의 리모트 서버(리모트 머신)을 태스크`라고 한다. EC2인스턴스와 비슷하지만 `직접 관리하지 않고 실행 방법, 환경 설정을 AWS에 알려주는게 태스크`다. 컨테이너를 시작하는 특정 방법은 디폴트로 FARGATE, `서버리스`라고 부르는 모드에서 구동된다. 그래서 컨테이너에 요청이 들어오면 컨테이너를 시작해 요청을 처리하고 다시 중지된다. 실행 시간에만 비용을 지불하므로 효율적이다. 태스크 크기는 컨테이너가 실행될 때 `사용할 수 있는 메모리, CPU 양`을 의미한다. 그런데 리소스가 충분하지 않은 시점이 오면 AWS ECS는 **Auto Scaling**을 제공해 동시에 둘 이상의 실행 중인 컨테이너를 생성한다. 그래서 들어오는 요청 처리가능 하도록 더 많은 컨테이너 생성에 도움이 된다.
  - `서비스 정의` 탭. 서비스는 태스크를 실행하는 방법을 컨트롤한다. 로드밸런서를 추가(수신 요청 리디렉션 등 백그라운드에서 관리)할 수도 있다. 모든 태스크는 서비스에 의해 실행된다.
  - `클러스터 구성` 탭. 서비스가 실행되는 전체 네트워크.
  - 소스 코드 업데이터를 반영하기 위해 빌드-푸시한 후 ECS-클러스터-default로 이동. TASK로 이동해 task definiton에서 클릭 후 create new revision에서 전에 만든 걸 그대로 다시 만들면 새 이미지를 사용한다. 그 다음 Actions - update service를 클릭한다.(새로 태스크 안만들려면 Update service - Force new Deployment) 그리고 skip to review - update service로 완료한다. 그리고 task를 보면 여전히 1버전도 실행중이지만 잠시 후 중단된다. 그리고 새로운 태스크의 퍼블릭 ip로 접속한다.
  - 더 이상 사용하지 않으면 클러스터에서 서비스를 삭제 한 후 클러스터도 삭제한다.
- AWS ECS: 다중 컨테이너 
  - 배포에 도커 컴포즈를 사용하지 않는다. 로컬에서 다중 컨테이너 실행을 쉽게 하지만 리모트 머신으로 이동하면 주어진 서비스에 대해 제공하는 cpu 용량을 정의해야 한다. 동일 머신 다중 컨테이너 관리와 실행에 중요하지만 여러 머신이 함께 작동할 클라우드로 이동하면 한계에 도달한다. 
  - 또 AWS ECS에서는 `컨테이너 이름으로 IP찾기 ` 기능을 사용할 수 없다. 로컬 머신에서 실행하면 한 컴퓨터에서 발생하지만 AWS ECS에서 클라우드 프로바이더에 의해 컨테이너와 인스턴스가 실행, 관리되는데 거대한 데이터 센터에서 동일한 머신에서 실행될 가능성이 거의 없다. 동일 태스크에 컨테이너를 추가하면 동일 머신 실행이 보장된다. 그래도 도커 네트워크를 생성하지 않고 대신 localhost를 내부 주소로 사용하게 해준다. 이럴 때 로컬과 프로덕션 환경을 동일하게 유지하기 위해 `환경 변수`를 사용한다. 
  - backend만 빌드해서 먼저 저장소에 푸시하는데 이 때 env_file은 docker-compose.yaml에 있지만 backend위치에 없고 Dockerfile에 정의되어있지만 .dockerignore에 포함되어있으므로 환경 변수는 제공되지 않는다. 
  - create cluster - network only - create vpc 체크(aws가 클러스터 모든 컨테이너에 대해 이 클러스터를 프라이빗 클라우드로 설정한다.)
  - 완료되면 작업 정의에서 새 작업 정의를 생성한다. fargate로 생성해 태스트 역할은 `ecsTaskExecutionRole(없으면 전체 클러스터 삭제하고 다시)` 그리고 컨테이너 추가는 앞의 방법과 같다. 다만 `환경`에서 개발 코드에서 nodemon을 이용해 시작하지만 배포 환경에서는 라이브 코드 실행이 필요하지 않으므로 node,app.js로 변경한다.
  - 그리고 dockerfile에 backend.env에만 존재하던 URL 환경변수를 추가하고 재 빌드해 푸시한 다음 컨테이너 생성하는 과정에서 환경변수에 추가한다. 이전엔 개발용으로 mongodb라는 이름을 url로 사용했지만 배포 과정에서는 동일 태스크에 추가되는 mongodb를 사용하기 위해 `localhost`로 지정한다. 
  - 오류나서 일단 패스...



### 섹션 11: Kubernetes 시작하기

- 클라우드 프로바이더와 상관없이 컨테이너 오케스트레이션, 대규모 배포에 도움이 되는 도구이자 표준 모음, 프레임워크. `컨테이너화된 애플리케이션의 배포, 확장, 관리를 자동화하기 위한 오픈 소스 시스템`
- 앞서 AWS EC2인스턴스를 생성했을 때 처럼 컨테이너를 수동 배포하면 인스턴스를 자체적으로 관리(보안), 구성하면서 소프트웨어와 운영체제가 업데이트 된 상태를 유지해야 한다. 또 인스턴스에서 컨테이너를 수동 실행했을 때 충돌, 다운되면 새 컨테이너로 교체해야 한다. 컨테이너 내부에서 문제가 발생하면 전체가 충돌해 교체하지 않으면 접근할 수 없게 된다.  결국 수동 배포 시 충돌이 발생하는지 모니터링하고있다가 다시 시작해야 한다. 뿐만 아니라 트래픽이 급증하면 다음 요청을 처리하기 전 첫 번째 태스크를 완료하기 위해 멈춰버릴 수 있어 더 많은 컨테이너 인스턴스가 필요하다.  따라서 ` 컨테이너 수를 증가시켜 트래픽을 줄이고 여러 컨테이너에 분산해야 한다. 그리고 트래픽이 줄어들면 컨테이너를 다시 제거한다. ` 웹 개발을 위한 컨테이너 뿐 아니라 애플리케이션 실행이나 이미지 파일 변환 태스크에서는 HTTP 요청만 처리하지 않고 파일 처리 후 업로드를 하기도 한다. 이런 요청이 많을 때 동일 태스크를 실행하는 여러 컨테이너를 구동하는 해결책이 있다. 지금까지 계속 이미지당 하나의 컨테이너를 만들었지만 동일한 이미지를 기반으로 여러 컨테이너 인스턴스를 만들어 작업을 분할하면 된다. `스케일링`. 그런데 이런 방법은 수동으로 배포할 때 문제가 될 수 있다.
- 이전에 사용했던 ECS는 실행중인지 컨테이너 상태를 확인하고 충돌하면 다시 시작한다. 오토 스케일링도 트래픽 급증하면 인스턴스 수를 늘이고 로드 밸런서로 테스트에 사용할 수 있는 변경되지 않는 IP 주소 또는 도메인을 갖도록 하며 인스턴스가 2개 이상이면 트래픽이 균등하게 분할되는지도 확인한다. 다만 이런 관리형 서비스를 사용하면 특정 클라우드 프로바이더에 고정된다. UI를 통해 설정하거나 ECS-CLI, 구성 파일을 사용할 수 있지만 결국 특정 서비스에 고정된 것이다. 다른 프로바이더로 전환할 때는 또 다른 서비스에 대해 알아야 한다. 그런데 이걸 쿠버네티스가 도와준다.
- 쿠버네티스를 통해 배포 방식, 컨테이너 스케일링, 모니터링 방법, 컨테이너 교체 방법을 정의할 수 있다. 그리고 이런 구성을 클라우드 프로바이더나 자체 머신(전용 데이터 센터)에 전달해 적용한다. 쿠버네티스를 지원하는 프로바이더에서 작동하고 자체 머신에 쿠버네티스를 수동으로 설치해 활용할 수 있다. 또한 특저어 클라우드 프로바이더에 특화된 옵션을 구성 파일에 추가할 수 있다. 다른 프로바이더로 전환하면 전체를 다시 작성할 필요없이 특정 방식을 교체,제거 한다.
- 쿠버네티스는 클라우드 프로바이더가 아니며 도커를 대체하는 도구도 아니다. 여러 머신을 위한 Docker-Compose같은 것이다. 
- 쿠버네티스는 `포드(Pod)`라는 것에 의해 관리된다. 가장 작은 단위로 구성 파일에서 정의할 수 있다. 포드는 컨테이너를 보유한다. 함께 작동해야 하는 여러 컨테이너를 보유할 수도 있으며 컨테이너 실행 책임이 있어 실행시킨다. 포드는 `워커(worker) 노드`에서 자신을 실행한다. 워커 노드를 `머신, 가상 인스턴스`로 생각할 수 있다. 예를들어 EC2 인스턴스가 워커 노드가 된다. 또 워커 노드에는 `프록시`가 필요하다. 프록시는 워커 노드에서 포드 네트워크 트래픽 제어를 설정하는 도구다. 포드가 `인터넷에 연결할 수 있는지`, `포드와 그 내부 컨테이너를 외부에서 어떻게 접근하는지를 제어`한다. 포드의 컨테이너에서 웹 애플리케이션을 실행할 경우 외부 트래픽이 컨테이너에 도달할 수 있도록 프록시를 구성해야 한다. 
  - 서비스: 특정 포드를 외부에 노출해 특정 ip주소나 도메인으로 특정 포드에 연결할 수 있도록하는 것. 
- 쿠버네티스에서 트래픽에 따라 컨테이너 및 포드를 동적으로 추가, 제거할 때 포드는 사용 가능한 워커 노드로 자동 배포된다. 그리고 여러 워커 노드에서 컨테이너를 실행해 워크 로드를 고르게 분배한다. 아것들을 제어해 만들거나 시작하고 실패하면 교체하거나 종료해야 한다. 이 일은 `마스터 노드`에 의해 호출되는 `컨트롤 플레인(plane)`에 의해 수행된다. `워커 노드와 상호작용해 제어하는 컨트롤 센터`다. 마스터 노드는 다른 서버인 리모트 머신으로 컨트롤 플레인을 가지며 워커노드와 그 안에서 실행중인 포드와 상호작용하는 책임을 가진다. 이론적으로는 하나의 머신을 가질 수 있지만 더 큰 배포에서는 자체적으로 여러 머신에 분할될 수 있는 마스터 노드가 있고 워커 노드는 다른 인스턴스, 마스터 노드와 독립적인 다른 머신이 된다. 따라서 한 워커 노드가 다운되어도 마스터 노드는 함께 다운되지 않는다. 
- 이 모든 것이 클러스터를 형성해 모든 부분이 연결된 하나의 네트워크를 형성한다. 
  - 노드 머신, 마스터 노드, 워커 노드, 배포 등 원하는 최종 상태를 구성하는 모든 것의 컬렉션
- 마스터 노드가 클라우드 프로바이더 api에 명령을 보내 특정 리소스를 생성하고 원하는 최종 상태를 클라우드 프로바이더에 복제하라고 지시한다. 
- 쿠버네티스를 사용하기 위해 클러스터, 워커 노트, 마스터 노드를 만들고 설정해야 한다. 그리고 노드(클러스터 일부인 머신)에 쿠버네티스 소프트웨어를 설치해야 한다. 클라우드 프로바이더에 따라 로드 밸런서나 특정 파일 시스템 서비스를 설정해야할 수도 있다. 
- 워커 노드
  - computer / machine/ virtual instance라고 생각
  - 마스터 노드에서 관리
  - 내부에 포드(하나 이상의 애플리케이션 컨테이너, 컨테이너에 속한 모든 리소스 호스팅 - 볼륨같은 것도 포함)
  - 워커 노드에는 둘 이상의 포드가 실행되는게 일반적, 다른 포드의 복사본일 수 있고(스케일링) 완전히 다른 작업 수행일 수도 있음(워커 노드는 태스크에 특정되지 않음. 그냥 우리 컴퓨터처럼 생각하면 된다.)
  - 포드 뿐 아니라 추가 소프트웨어 존재 - 도커를 여기 설치, kublet(워커와 마스터 통신 장치로 마스터가 워커 노드 포드 제어하도록 한다.), 프록시 서브(들어오고 나가는 트래픽 처리)
  - aws는 쿠버네티스 정의(원하는 최종 상태)를 제공하는 서비스가 존재해 모든 인스턴스를 설정하고 설치한다.
- 마스터 노드
  - 가장 중요한 서비스: 구동되는 API 서버. 워커 노드의 kublet과 통신. 
  - scheduler: 포드 관찰, 새 포드 생성되어야 하는 워커 노드 선택(다운되었거나 스케일링때문에) => 워커 노드에 알려야하는 것을 api 서버에 알려줌
  - kube-controller-manager: 워커 노드 전체 감시하고 적당한 수의 포드가 가동중인지 확인. => 스케줄러, api서비스와 연동
    - cloud-controller-manager: kube-controller-manager의 특정 버전. 동일한 작업을 수행하지만 클라우드 프로바이더에 따라 다르게 명령을 번역해준다. 
